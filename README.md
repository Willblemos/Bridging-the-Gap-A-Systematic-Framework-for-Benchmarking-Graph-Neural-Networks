# Bridging-the-Gap-A-Systematic-Framework-for-Benchmarking-Graph-Neural-Networks

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC/7tafuXVQOl/N9EzKf2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Willblemos/Bridging-the-Gap-A-Systematic-Framework-for-Benchmarking-Graph-Neural-Networks/blob/main/Bridging_the_Gap_A_Systematic_Framework_for_Benchmarking_Graph_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Gp_GDHlLi6d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html}\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCTs_tZIi9EZ",
        "outputId": "bd46bf69-6e71-414f-b779-6cb81baeb085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Embedding Benchmark"
      ],
      "metadata": {
        "id": "AyeQGG22i_tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.datasets import KarateClub, Planetoid\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Linear\n",
        "\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset\n",
        "from sklearn.manifold import TSNE  # For dimensionality reduction\n",
        "import time\n",
        "\n",
        "\n",
        "# Variável global para armazenar os resultados\n",
        "results_df = pd.DataFrame(columns=['Dataset', 'Nodes', 'Edges', 'Model', 'Num Layers', 'Num Neurons', 'Learning Rate', 'Dropout', 'Train Accuracy', 'Test Accuracy','Execution Time', 'Setup'])\n",
        "\n",
        "# Load DATASETS ###########################################################\n",
        "KarateClub_dataset = KarateClub()\n",
        "Cora_dataset = Planetoid(root='data/Cora', name='Cora')\n",
        "Citeseer_dataset = Planetoid(root='data/Planetoid', name='Citeseer')\n",
        "Pubmed_dataset = Planetoid(root='data/Planetoid', name='Pubmed')\n",
        "# Synthetic_pattern_dataset = GNNBenchmarkDataset(root='data/GNNBenchmark', name='PATTERN', split='train')\n",
        "# Synthetic_cluster_dataset = GNNBenchmarkDataset(root='data/GNNBenchmark', name='CLUSTER', split='train')\n",
        "\n",
        "# datasets = [Cora_dataset,Citeseer_dataset,Pubmed_dataset,Synthetic_pattern_dataset,Synthetic_cluster_dataset]\n",
        "# datasets_names = ['Cora', 'CiteSeer', 'Pubmed', 'Syntethic_pattern', 'Synthetic_cluster']\n",
        "\n",
        "datasets = [Cora_dataset,Citeseer_dataset,Pubmed_dataset]\n",
        "datasets_names = ['Cora', 'CiteSeer', 'Pubmed']\n",
        "\n",
        "for graph in datasets:\n",
        "  dataset_name = datasets_names[datasets.index(graph)]\n",
        "  dataset = graph\n",
        "\n",
        "\n",
        "  # Parâmetros gerais\n",
        "  num_layers_values = [1,2, 3]\n",
        "  num_neurons_values = [16, 32, 64]\n",
        "  num_epochs = 150\n",
        "  learning_rate_values = [0.01, 0.001, 0.0001]\n",
        "  dropout_values = [0.5, 0.2, 0.1]  # Variação do dropout\n",
        "\n",
        "  # Recuperando informações do conjunto de dados\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  num_nodes = dataset.data.num_nodes\n",
        "  num_edges = dataset.data.num_edges\n",
        "\n",
        "  # Separando os dados em treino e teste\n",
        "  data = dataset[0]\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  data = data.to(device)\n",
        "  x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "  # Definindo o modelo GCN\n",
        "  class GCN(torch.nn.Module):\n",
        "      def __init__(self, num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "          super(GCN, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(GCNConv(num_features, num_neurons))\n",
        "          for _ in range(num_layers - 1):\n",
        "              self.convs.append(GCNConv(num_neurons, num_neurons))\n",
        "          self.output = GCNConv(num_neurons, num_classes)\n",
        "          self.activation = activation\n",
        "          self.dropout = dropout\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index)\n",
        "              x = self.activation(x)\n",
        "              x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "          x = self.output(x, edge_index)\n",
        "          return F.log_softmax(x, dim=1)\n",
        "\n",
        "  # Definindo o modelo GraphSAGE\n",
        "  class GraphSAGE(torch.nn.Module):\n",
        "      def __init__(self, num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "          super(GraphSAGE, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(SAGEConv(num_features, num_neurons, normalize=False))\n",
        "          for _ in range(num_layers - 1):\n",
        "              self.convs.append(SAGEConv(num_neurons, num_neurons, normalize=False))\n",
        "          self.output = SAGEConv(num_neurons, num_classes, normalize=False)\n",
        "          self.activation = activation\n",
        "          self.dropout = dropout\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index)\n",
        "              x = self.activation(x)\n",
        "              x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "          x = self.output(x, edge_index)\n",
        "          return F.log_softmax(x, dim=1)\n",
        "\n",
        "  # Definindo o modelo GAT\n",
        "  class GAT(torch.nn.Module):\n",
        "      def __init__(self, num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "          super(GAT, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(GATConv(num_features, num_neurons))\n",
        "          for _ in range(num_layers - 1):\n",
        "              self.convs.append(GATConv(num_neurons, num_neurons))\n",
        "          self.output = GATConv(num_neurons, num_classes)\n",
        "          self.activation = activation\n",
        "          self.dropout = dropout\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index)\n",
        "              x = self.activation(x)\n",
        "              x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "          x = self.output(x, edge_index)\n",
        "          return F.log_softmax(x, dim=1)\n",
        "\n",
        "  # Função para criar e treinar um modelo\n",
        "  def train_model(model, optimizer, criterion, x, edge_index, y, data, num_epochs, model_name, num_layers, num_neurons, learning_rate, dropout):\n",
        "      print(f'\\nTraining {model_name} model...')\n",
        "      print(f'Number of layers: {num_layers}, Number of neurons: {num_neurons}, Epochs: {num_epochs}, Learning Rate: {learning_rate}, Dropout: {dropout}')\n",
        "\n",
        "      train_losses, test_losses = [], []\n",
        "      train_accuracies, test_accuracies = [], []\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          model.train()\n",
        "          optimizer.zero_grad()\n",
        "          out = model(x, edge_index)\n",
        "          loss = criterion(out[data.train_mask], y[data.train_mask])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_losses.append(loss.item())\n",
        "\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              out = model(x, edge_index)\n",
        "              test_loss = criterion(out[data.test_mask], y[data.test_mask])\n",
        "              test_losses.append(test_loss.item())\n",
        "\n",
        "              _, predicted = torch.max(out[data.test_mask], 1)\n",
        "              total = data.test_mask.sum().item()\n",
        "              correct = predicted.eq(y[data.test_mask]).sum().item()\n",
        "              test_accuracy = correct / total\n",
        "              test_accuracies.append(test_accuracy)\n",
        "\n",
        "          _, predicted = torch.max(out[data.train_mask], 1)\n",
        "          total = data.train_mask.sum().item()\n",
        "          correct = predicted.eq(y[data.train_mask]).sum().item()\n",
        "          train_accuracy = correct / total\n",
        "          train_accuracies.append(train_accuracy)\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "                f'Training Loss: {train_losses[-1]:.4f}, '\n",
        "                f'Test Loss: {test_losses[-1]:.4f}, '\n",
        "                f'Training Accuracy: {train_accuracies[-1]:.4f}, '\n",
        "                f'Test Accuracy: {test_accuracies[-1]:.4f}')\n",
        "\n",
        "\n",
        "      return train_losses, test_losses, train_accuracies, test_accuracies\n",
        "\n",
        "  # Função para criar o modelo GCN\n",
        "  def create_gcn_model(num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "      model = GCN(num_features, num_classes, num_layers, num_neurons, activation, dropout).to(device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      criterion = torch.nn.NLLLoss()\n",
        "      return model, optimizer, criterion\n",
        "\n",
        "  # Função para criar o modelo GraphSAGE\n",
        "  def create_graphsage_model(num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "      model = GraphSAGE(num_features, num_classes, num_layers, num_neurons, activation, dropout).to(device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      criterion = torch.nn.NLLLoss()\n",
        "      return model, optimizer, criterion\n",
        "\n",
        "  # Função para criar o modelo GAT\n",
        "  def create_gat_model(num_features, num_classes, num_layers, num_neurons, activation, dropout):\n",
        "      model = GAT(num_features, num_classes, num_layers, num_neurons, activation, dropout).to(device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      criterion = torch.nn.NLLLoss()\n",
        "      return model, optimizer, criterion\n",
        "\n",
        "  # Função para armazenar os resultados no dataframe global\n",
        "  def store_results(dataset_name, num_nodes, num_edges, model_name, num_layers, num_neurons, learning_rate, dropout,\n",
        "                    train_accuracy, test_accuracy, execution_time, setup):\n",
        "      global results_df\n",
        "      results_df = results_df.append({\n",
        "          'Dataset': dataset_name,\n",
        "          'Nodes': num_nodes,\n",
        "          'Edges': num_edges,\n",
        "          'Model': model_name,\n",
        "          'Num Layers': num_layers,\n",
        "          'Num Neurons': num_neurons,\n",
        "          'Learning Rate': learning_rate,\n",
        "          'Dropout': dropout,\n",
        "          'Train Accuracy': train_accuracy,\n",
        "          'Test Accuracy': test_accuracy,\n",
        "          'Execution Time': execution_time,\n",
        "          'Setup': setup\n",
        "      }, ignore_index=True)\n",
        "\n",
        "  # Loop para executar o experimento com diferentes hiperparâmetros\n",
        "  experiment_num = 1\n",
        "\n",
        "  for num_layers in num_layers_values:\n",
        "      for num_neurons in num_neurons_values:\n",
        "          for learning_rate in learning_rate_values:\n",
        "              for dropout in dropout_values:  # Variação do dropout\n",
        "                  start_time = time.time()\n",
        "                  # Criando e treinando o modelo GCN\n",
        "                  gcn_model, gcn_optimizer, gcn_criterion = create_gcn_model(num_features, num_classes, num_layers, num_neurons, F.relu, dropout)  # Dropout passado para a criação do modelo\n",
        "                  gcn_train_losses, gcn_test_losses, gcn_train_accuracies, gcn_test_accuracies = train_model(\n",
        "                      gcn_model, gcn_optimizer, gcn_criterion, x, edge_index, y, data, num_epochs, 'GCN', num_layers, num_neurons, learning_rate, dropout\n",
        "                  )\n",
        "                  end_time = time.time()\n",
        "                  gcn_execution_time = end_time - start_time\n",
        "                  store_results(dataset_name, num_nodes, num_edges, 'GCN', num_layers, num_neurons, learning_rate, dropout,\n",
        "                                gcn_train_accuracies[-1], gcn_test_accuracies[-1], gcn_execution_time ,experiment_num)\n",
        "\n",
        "                  # Criando e treinando o modelo GraphSAGE\n",
        "                  start_time = time.time()\n",
        "                  graphsage_model, graphsage_optimizer, graphsage_criterion = create_graphsage_model(\n",
        "                      num_features, num_classes, num_layers, num_neurons, F.relu, dropout\n",
        "                  )\n",
        "                  graphsage_train_losses, graphsage_test_losses, graphsage_train_accuracies, graphsage_test_accuracies = train_model(\n",
        "                      graphsage_model, graphsage_optimizer, graphsage_criterion, x, edge_index, y, data, num_epochs, 'GraphSAGE', num_layers, num_neurons, learning_rate, dropout\n",
        "                  )\n",
        "                  end_time = time.time()\n",
        "                  sage_execution_time = end_time - start_time\n",
        "                  store_results(dataset_name, num_nodes, num_edges, 'GraphSAGE', num_layers, num_neurons, learning_rate, dropout,\n",
        "                                graphsage_train_accuracies[-1], graphsage_test_accuracies[-1], sage_execution_time, experiment_num)\n",
        "\n",
        "                  # Criando e treinando o modelo GAT\n",
        "                  start_time = time.time()\n",
        "                  gat_model, gat_optimizer, gat_criterion = create_gat_model(num_features, num_classes, num_layers, num_neurons, F.elu, dropout)\n",
        "                  gat_train_losses, gat_test_losses, gat_train_accuracies, gat_test_accuracies = train_model(\n",
        "                      gat_model, gat_optimizer, gat_criterion, x, edge_index, y, data, num_epochs, 'GAT', num_layers, num_neurons, learning_rate, dropout\n",
        "                  )\n",
        "                  end_time = time.time()\n",
        "                  gat_execution_time = end_time - start_time\n",
        "                  store_results(dataset_name, num_nodes, num_edges, 'GAT', num_layers, num_neurons, learning_rate, dropout,\n",
        "                                gat_train_accuracies[-1], gat_test_accuracies[-1], gat_execution_time,experiment_num)\n",
        "\n",
        "                  experiment_num += 1\n",
        "\n",
        "# Salvar os resultados em um arquivo CSV\n",
        "results_df.to_csv('results.csv', index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "nil6Ue7TjE9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Analysis"
      ],
      "metadata": {
        "id": "OTYhG-y7jV1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.datasets import KarateClub, Planetoid\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Linear\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset\n",
        "\n",
        "from sklearn.manifold import TSNE  # For dimensionality reduction\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load DATASETS ###########################################################\n",
        "KarateClub_dataset = KarateClub()\n",
        "Cora_dataset = Planetoid(root='data/Cora', name='Cora')\n",
        "Citeseer_dataset = Planetoid(root='data/Planetoid', name='Citeseer')\n",
        "Pubmed_dataset = Planetoid(root='data/Planetoid', name='Pubmed')\n",
        "Synthetic_pattern_dataset = GNNBenchmarkDataset(root='data/GNNBenchmark', name='PATTERN', split='train')\n",
        "Synthetic_cluster_dataset = GNNBenchmarkDataset(root='data/GNNBenchmark', name='CLUSTER', split='train')\n",
        "# Create an empty list to store the results for each model\n",
        "results = []\n",
        "metrics = []\n",
        "\n",
        "#dataset = Pubmed_dataset\n",
        "\n",
        "#datasets = [Cora_dataset,Citeseer_dataset,Synthetic_pattern_dataset,Synthetic_cluster_dataset]\n",
        "#datasets = [Cora_dataset,Citeseer_dataset]\n",
        "datasets = [Synthetic_pattern_dataset,Synthetic_cluster_dataset]\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "  # Print information about the dataset\n",
        "  print('------------------------------------------------------------------------------------')\n",
        "  print(f\"Information about Dataset: {dataset}\")\n",
        "  print('------------')\n",
        "  print(f'Number of graphs: {len(dataset)}')\n",
        "  print(f'Number of features: {dataset.num_features}')\n",
        "  print(f'Number of classes: {dataset.num_classes}')\n",
        "  print(f'Number of nodes (vertices): {dataset[0].num_nodes}')\n",
        "  print(f'Number of edges: {dataset[0].num_edges}')\n",
        "  print('------------------------------------------------------------------------------------')\n",
        "\n",
        "  data = dataset[0]\n",
        "  A = to_dense_adj(data.edge_index)[0].numpy().astype(int)\n",
        "\n",
        "  # Set hyperparameters\n",
        "  num_features = dataset.num_features\n",
        "  num_classes = dataset.num_classes\n",
        "  lr = 0.0001\n",
        "  num_epochs = 50\n",
        "  num_hidden_channels = 8\n",
        "  num_layers = 1\n",
        "\n",
        "  G = to_networkx(data, to_undirected=True)\n",
        "  reconstructed_embeddings = []\n",
        "\n",
        "  # Define GCN model\n",
        "  class GCN(torch.nn.Module):\n",
        "      def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
        "          super(GCN, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(GCNConv(num_features, hidden_channels))\n",
        "          for i in range(num_layers - 1):\n",
        "              self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "          self.out = Linear(hidden_channels, num_classes)\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index).relu()\n",
        "          z = self.out(x)\n",
        "          return x, z\n",
        "\n",
        "  # Define GraphSAGE model\n",
        "  class GraphSAGE(torch.nn.Module):\n",
        "      def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
        "          super(GraphSAGE, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(SAGEConv(num_features, hidden_channels))\n",
        "          for i in range(num_layers - 1):\n",
        "              self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "          self.out = Linear(hidden_channels, num_classes)\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index).relu()\n",
        "          z = self.out(x)\n",
        "          return x, z\n",
        "\n",
        "  # Define GAT model\n",
        "  class GAT(torch.nn.Module):\n",
        "      def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
        "          super(GAT, self).__init__()\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(GATConv(num_features, hidden_channels))\n",
        "          for i in range(num_layers - 1):\n",
        "              self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
        "          self.out = Linear(hidden_channels, num_classes)\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          for conv in self.convs:\n",
        "              x = conv(x, edge_index).relu()\n",
        "          z = self.out(x)\n",
        "          return x, z\n",
        "\n",
        "  # Function to calculate accuracy\n",
        "  def accuracy(pred_y, y):\n",
        "      return (pred_y == y).sum() / len(y)\n",
        "\n",
        "\n",
        "\n",
        "  # Training loop function\n",
        "  def train_model(model, data, criterion, optimizer, num_epochs=201):\n",
        "      embeddings = []\n",
        "      losses = []\n",
        "      accuracies = []\n",
        "      outputs = []\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          # Clear gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          h, z = model(data.x, data.edge_index)\n",
        "\n",
        "          # Calculate loss function\n",
        "          loss = criterion(z, data.y)\n",
        "\n",
        "          # Calculate accuracy\n",
        "          acc = accuracy(z.argmax(dim=1), data.y)\n",
        "\n",
        "          # Compute gradients\n",
        "          loss.backward()\n",
        "\n",
        "          # Tune parameters\n",
        "          optimizer.step()\n",
        "\n",
        "          # Store data for animations\n",
        "          embeddings.append(h)\n",
        "          losses.append(loss.item())\n",
        "          accuracies.append(acc.item())\n",
        "          outputs.append(z.argmax(dim=1))\n",
        "\n",
        "          # Print metrics every 10 epochs\n",
        "          if epoch % 10 == 0:\n",
        "              print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')\n",
        "\n",
        "      return embeddings, losses, accuracies, outputs\n",
        "\n",
        "\n",
        "  # Function to train and evaluate a model\n",
        "  def train_and_evaluate_model(model, model_name, data, criterion, optimizer, num_epochs=201):\n",
        "      print(f\"\\nTraining {model_name}...\")\n",
        "      print(f\"Dataset: {dataset}\")\n",
        "      print(f\"Number of Features: {num_features}, Hidden Channels: {num_hidden_channels}, Number of Classes: {num_classes}, Learning Rate: {lr}, Number of Epochs: {num_epochs}, Number of Layers: {num_layers}\")\n",
        "\n",
        "      start_time = time.time()\n",
        "      mem_usage = []\n",
        "\n",
        "      embeddings, losses, accuracies, outputs = train_model(model, data, criterion, optimizer, num_epochs=num_epochs)\n",
        "      end_time = time.time()\n",
        "      execution_time = end_time - start_time\n",
        "\n",
        "      # Calculate assortativity coefficient based on the last graph embedding\n",
        "      final_h = embeddings[-1]\n",
        "      #final_h_np = final_h.detach().cpu().numpy()\n",
        "      #final_G = nx.Graph(nx.from_numpy_array(np.dot(final_h_np, final_h_np.T)))\n",
        "      # original_metrics = Graph_metrics(G)\n",
        "      # embedded_metrics = Graph_metrics(final_G)\n",
        "      reconstructed_embeddings.append(final_h)\n",
        "\n",
        "      # Print metrics\n",
        "      print(f\"\\nMetrics for {model_name}:\")\n",
        "      print(f\"Final Loss: {losses[-1]:.4f}\")\n",
        "      print(f\"Final Accuracy: {accuracies[-1] * 100:.2f}%\")\n",
        "      print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "      #return losses, accuracies, outputs, original_metrics, embedded_metrics, execution_time\n",
        "      return losses, accuracies, outputs, execution_time, final_h\n",
        "\n",
        "\n",
        "  # Initialize models and optimizer\n",
        "  gcn_model = GCN(num_features, num_hidden_channels, num_classes, num_layers)\n",
        "  graphsage_model = GraphSAGE(num_features, num_hidden_channels, num_classes, num_layers)\n",
        "  gat_model = GAT(num_features, num_hidden_channels, num_classes, num_layers)\n",
        "\n",
        "  models = [gcn_model, graphsage_model, gat_model]\n",
        "  model_names = ['GCN', 'GraphSAGE', 'GAT']\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train and evaluate models\n",
        "  for model, model_name in zip(models, model_names):\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "      #losses, accuracies, outputs, original_metrics, embedded_metrics,execution_time = train_and_evaluate_model(model, model_name, data, criterion, optimizer, num_epochs=num_epochs)\n",
        "      losses, accuracies, outputs, execution_time, embeddings = train_and_evaluate_model(model, model_name, data, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "      # Add the model results to the list\n",
        "      results.append({\n",
        "          \"Model\": model_name,\n",
        "          \"Dataset\": str(dataset),\n",
        "          \"Number of Features\": num_features,\n",
        "          \"Hidden Channels\": num_hidden_channels,\n",
        "          \"Number of Classes\": num_classes,\n",
        "          \"Learning Rate\": lr,\n",
        "          \"Number of Epochs\": num_epochs,\n",
        "          \"Number of Layers\": num_layers,\n",
        "          \"Final Loss\": losses[-1],\n",
        "          \"Final Accuracy\": accuracies[-1] * 100,\n",
        "          \"Execution Time (seconds)\": execution_time,\n",
        "      })\n",
        "\n",
        "      metrics.append([str(dataset),model_name, embeddings])\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "embedding_results = pd.DataFrame(results)\n",
        "\n",
        "def reconstruct_graph(embedded_graph):\n",
        "  final_h_np = embedded_graph.detach().cpu().numpy()\n",
        "  reconstructed_graph = nx.Graph(nx.from_numpy_array(np.dot(final_h_np, final_h_np.T)))\n",
        "  return (reconstructed_graph)\n",
        "\n",
        "\n",
        "def Graph_metrics(embedding_output):\n",
        "    print(\"***** Running metrics for: \" + str(embedding_output[0]))\n",
        "    graph = reconstruct_graph(embedding_output[2])\n",
        "    assortativity_coefficient = nx.degree_assortativity_coefficient(graph)\n",
        "    global_efficiency = nx.global_efficiency(graph)\n",
        "    transitivity = nx.transitivity(graph)\n",
        "    triadic_census = 0\n",
        "    motif_counts = 0\n",
        "    clustering_coefficient = 0\n",
        "\n",
        "    return [\n",
        "        embedding_output[0],\n",
        "        embedding_output[1],\n",
        "        assortativity_coefficient,\n",
        "        global_efficiency,\n",
        "        #triadic_census,\n",
        "        #motif_counts,\n",
        "        #clustering_coefficient,\n",
        "        transitivity\n",
        "    ]\n",
        "\n",
        "network_results = []\n",
        "for model in metrics:\n",
        "  print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "  dataset,model_name,assortativity_coefficient,global_efficiency,transitivity = Graph_metrics(model)\n",
        "  network_results.append({\n",
        "    \"Dataset\": str(dataset),\n",
        "    \"Model\": model_name,\n",
        "    \"assortativity_coefficient\": assortativity_coefficient,\n",
        "    \"global_efficiency\": global_efficiency,\n",
        "    \"transitivity\": transitivity,\n",
        "})\n",
        "  print(\"-----------------------------------------------------------------------------\")\n",
        "network_analysis = pd.DataFrame(network_results)\n",
        "network_analysis\n"
      ],
      "metadata": {
        "id": "MlirsUjKjYT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis and Visualization"
      ],
      "metadata": {
        "id": "1Xt6cyxfj-gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pre-calculated results (saving time)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "data = pd.read_csv (r'/content/drive/MyDrive/Graph_experiments/results_full.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "x1Hfv-X3kDZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Via in-memory results:\n",
        "data = results_df\n",
        "data"
      ],
      "metadata": {
        "id": "ysHguIM9kVpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box-plot and Point results"
      ],
      "metadata": {
        "id": "-631GbeLko7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data = results_df\n",
        "\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=[\n",
        "    \"Dataset\", \"Nodes\", \"Edges\", \"Model\", \"Num Layers\", \"Num Neurons\", \"Learning Rate\",\n",
        "    \"Dropout\", \"Train Accuracy\", \"Test Accuracy\", \"Setup\"\n",
        "])\n",
        "\n",
        "# Set up the plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Relationship between Learning Rate and Test Accuracy\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.scatterplot(x=\"Learning Rate\", y=\"Test Accuracy\", data=df, hue=\"Model\", legend=\"full\")\n",
        "plt.title(\"Learning Rate vs. Test Accuracy\")\n",
        "\n",
        "# Relationship between Dropout and Test Accuracy\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.scatterplot(x=\"Dropout\", y=\"Test Accuracy\", data=df, hue=\"Model\", legend=\"full\")\n",
        "plt.title(\"Dropout vs. Test Accuracy\")\n",
        "\n",
        "# Performance comparison among different GNN models\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.boxplot(x=\"Model\", y=\"Test Accuracy\", data=df)\n",
        "plt.title(\"Performance Comparison among GNN Models\")\n",
        "\n",
        "# Relationship between the number of neurons and Test Accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.scatterplot(x=\"Num Neurons\", y=\"Test Accuracy\", data=df, hue=\"Model\", legend=\"full\")\n",
        "plt.title(\"Number of Neurons vs. Test Accuracy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "il4-d4mDkjxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Accuracy by results"
      ],
      "metadata": {
        "id": "c_hGQPiMlG6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = results_df\n",
        "\n",
        "# Suponha que os dados estejam em um DataFrame chamado 'data'\n",
        "\n",
        "# Calcular a acurácia média de teste para cada setup\n",
        "mean_test_accuracy = data.groupby('Setup')['Test Accuracy'].mean()\n",
        "\n",
        "# Ordenar os índices pela acurácia média de teste em ordem crescente\n",
        "mean_test_accuracy_sorted = mean_test_accuracy.sort_values()\n",
        "\n",
        "# Plotar os resultados ordenados\n",
        "plt.figure(figsize=(20, 6))\n",
        "mean_test_accuracy_sorted.plot(kind='bar', color='blue')\n",
        "plt.title('Average Accuracy by setup')\n",
        "plt.xlabel('Setup')\n",
        "plt.ylabel('Avg test accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gqqwNNfUlJSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Sensibility"
      ],
      "metadata": {
        "id": "Q4bFIwgalMhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = results_df\n",
        "\n",
        "# Lista de modelos únicos\n",
        "models = data['Model'].unique()\n",
        "\n",
        "# Definir a paleta de cores\n",
        "palette = sns.color_palette(\"husl\", n_colors=len(models))\n",
        "\n",
        "# Lista de hiperparâmetros\n",
        "hyperparams = ['Num Layers', 'Num Neurons', 'Learning Rate', 'Dropout']\n",
        "\n",
        "# Loop pelos hiperparâmetros\n",
        "for param in hyperparams:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        # Filtrar dados para o modelo atual\n",
        "        model_data = data[data['Model'] == model]\n",
        "\n",
        "        # Agrupar os dados por hiperparâmetro e calcular as médias das métricas\n",
        "        hyperparams_grouped_data = model_data.groupby(param).mean()\n",
        "\n",
        "        # Cor baseada na paleta de cores e no índice do modelo\n",
        "        color = palette[i]\n",
        "\n",
        "        # Plotar linha de sensibilidade para o modelo atual\n",
        "        sns.lineplot(data=hyperparams_grouped_data.reset_index(), x=param, y='Test Accuracy', color=color, label=model)\n",
        "\n",
        "    plt.xlabel(param)\n",
        "    plt.ylabel('Avg Test accuracy')\n",
        "    plt.title(f'Accuracy sensibility to {param}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Definir os tickmarks com base nos valores reais presentes nos dados\n",
        "    plt.xticks(hyperparams_grouped_data.index)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qlWcLI1FlsqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark Summary results"
      ],
      "metadata": {
        "id": "ME1AGleGmCh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = results_df\n",
        "\n",
        "# Criar um DataFrame para armazenar os resultados\n",
        "summary_data = []\n",
        "\n",
        "# Para cada modelo, calcular as métricas e adicionar ao DataFrame\n",
        "for model in data['Model'].unique():\n",
        "    model_data = data[data['Model'] == model]\n",
        "    datasets = model_data['Dataset'].unique()\n",
        "    for dataset in datasets:\n",
        "        dataset_data = model_data[model_data['Dataset'] == dataset]\n",
        "        model_summary = {\n",
        "            'Model': model,\n",
        "            'Dataset': dataset,\n",
        "            'Min Test Accuracy': dataset_data['Test Accuracy'].min(),\n",
        "            'Max Test Accuracy': dataset_data['Test Accuracy'].max(),\n",
        "            'Mean Test Accuracy': dataset_data['Test Accuracy'].mean(),\n",
        "            'Standard Deviation': dataset_data['Test Accuracy'].std()\n",
        "        }\n",
        "        summary_data.append(model_summary)\n",
        "\n",
        "# Criar o DataFrame final com os resultados\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Exibir o DataFrame\n",
        "summary_df"
      ],
      "metadata": {
        "id": "9YL3dTEdmGfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = results_df\n",
        "\n",
        "def plot_accuracy_comparison(data, model_name):\n",
        "    # Filtrar os dados apenas para o modelo especificado\n",
        "    model_data = data[data['Model'] == model_name]\n",
        "\n",
        "    # Agrupar os dados por dataset e calcular as médias das métricas para o modelo especificado\n",
        "    grouped_data = model_data.groupby('Dataset').mean()\n",
        "\n",
        "    # Plotar gráfico de barras para as médias de acurácia de treinamento e teste para o modelo especificado\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    ax = grouped_data[['Train Accuracy', 'Test Accuracy']].plot(kind='bar')\n",
        "    plt.title(f'Average Train and Testing accuracy for {model_name} by dataset')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Adicionar os valores acima das barras\n",
        "    for i, value in enumerate(grouped_data['Train Accuracy']):\n",
        "        ax.text(i - 0.15, value + 0.01, f'{value:.3f}', color='black', fontsize=10)\n",
        "\n",
        "    for i, value in enumerate(grouped_data['Test Accuracy']):\n",
        "        ax.text(i + 0.15, value + 0.01, f'{value:.3f}', color='black', fontsize=10)\n",
        "\n",
        "    # Reposicionar a legenda fora da imagem\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Lista dos modelos presentes nos dados\n",
        "model_list = data['Model'].unique()\n",
        "\n",
        "# Para cada modelo, plote o gráfico de comparação de acurácia\n",
        "for model in model_list:\n",
        "    plot_accuracy_comparison(data, model)"
      ],
      "metadata": {
        "id": "mPLrtEGYmcls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
